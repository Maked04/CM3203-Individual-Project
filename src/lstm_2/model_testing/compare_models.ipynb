{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08161524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.models import load_model # type: ignore\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Local imports\n",
    "from src.lstm_2.model_generation.helper_methods import generate_data\n",
    "from src.data_processing.lstm_data_preprocessing import FeaturesConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07a74b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss function\n",
    "def weighted_mse_large_moves(y_true, y_pred):\n",
    "    diff = y_true - y_pred\n",
    "    weight = tf.math.square(y_true)\n",
    "    return tf.reduce_mean(weight * tf.square(diff))\n",
    "\n",
    "# Empty dictionary to hold all models\n",
    "models = {}\n",
    "\n",
    "# Create placeholder variable names (or models if already trained)\n",
    "batch_sizes = [16, 32, 64, 128, 256]\n",
    "\n",
    "for feature_set in range(1, 4):  # 1 to 3\n",
    "    for batch_size in batch_sizes:\n",
    "        model_folder = f\"../model_generation/trained_models/lstm_{feature_set}_{batch_size}\"\n",
    "        model_name = f\"model_{feature_set}_{batch_size}\"\n",
    "        with open(os.path.join(model_folder, \"config.json\"), 'r') as f:\n",
    "            configs = json.load(f)\n",
    "\n",
    "        features_config = configs[\"features_config\"]\n",
    "        time_bucket_folder = configs[\"time_bucket_folder\"]\n",
    "        test_size = configs[\"test_size\"]\n",
    "        model = load_model(os.path.join(model_folder, \"model.keras\"),\n",
    "            custom_objects={'weighted_mse_large_moves': weighted_mse_large_moves})\n",
    "        models[model_name] = {\"model\": model, \"feature_config\": features_config, \"time_bucket_folder\": time_bucket_folder, \"test_size\": test_size}\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9bf744b",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 765. MiB for an array with shape (25063500, 4) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m features_config \u001b[38;5;241m=\u001b[39m FeaturesConfig(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_config\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Generate/load your train/test data\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m X_train, X_test, y_train, y_test, X_scaler, y_scaler \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_bucket_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_bucket_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_size\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m datasets[config] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_train\u001b[39m\u001b[38;5;124m\"\u001b[39m: X_train,\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_test\u001b[39m\u001b[38;5;124m\"\u001b[39m: X_test,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_scaler\u001b[39m\u001b[38;5;124m\"\u001b[39m: y_scaler\n\u001b[0;32m     39\u001b[0m }\n",
      "File \u001b[1;32m~\\Uni Work\\CM3203 - Individual Project\\Project Environment\\CM3203-Individual-Project\\src\\lstm_2\\model_generation\\helper_methods.py:35\u001b[0m, in \u001b[0;36mgenerate_data\u001b[1;34m(features_config, time_bucket_folder, test_size)\u001b[0m\n\u001b[0;32m     33\u001b[0m num_samples, time_steps, features \u001b[38;5;241m=\u001b[39m all_X\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     34\u001b[0m X_reshaped \u001b[38;5;241m=\u001b[39m all_X\u001b[38;5;241m.\u001b[39mreshape(num_samples \u001b[38;5;241m*\u001b[39m time_steps, features)\n\u001b[1;32m---> 35\u001b[0m X_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mX_scaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_reshaped\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m X_scaled \u001b[38;5;241m=\u001b[39m X_scaled\u001b[38;5;241m.\u001b[39mreshape(num_samples, time_steps, features)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Scale target variable also using StandardScaler to preserve direction\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\matth\\Uni Work\\CM3203 - Individual Project\\Project Environment\\CM3203-Venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\matth\\Uni Work\\CM3203 - Individual Project\\Project Environment\\CM3203-Venv\\Lib\\site-packages\\sklearn\\base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    903\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    904\u001b[0m             (\n\u001b[0;32m    905\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    913\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m    914\u001b[0m         )\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\matth\\Uni Work\\CM3203 - Individual Project\\Project Environment\\CM3203-Venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:894\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    892\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\matth\\Uni Work\\CM3203 - Individual Project\\Project Environment\\CM3203-Venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\matth\\Uni Work\\CM3203 - Individual Project\\Project Environment\\CM3203-Venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1016\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_seen_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39misnan(X)\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   1015\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1016\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvar_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_seen_ \u001b[38;5;241m=\u001b[39m \u001b[43m_incremental_mean_and_var\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_samples_seen_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m            \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# for backward-compatibility, reduce n_samples_seen_ to an integer\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;66;03m# if the number of samples is the same for each feature (i.e. no\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;66;03m# missing values)\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mptp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_seen_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\matth\\Uni Work\\CM3203 - Individual Project\\Project Environment\\CM3203-Venv\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1107\u001b[0m, in \u001b[0;36m_incremental_mean_and_var\u001b[1;34m(X, last_mean, last_variance, last_sample_count, sample_weight)\u001b[0m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1106\u001b[0m     T \u001b[38;5;241m=\u001b[39m new_sum \u001b[38;5;241m/\u001b[39m new_sample_count\n\u001b[1;32m-> 1107\u001b[0m     temp \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\n\u001b[0;32m   1108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;66;03m# equivalent to np.nansum((X-T)**2 * sample_weight, axis=0)\u001b[39;00m\n\u001b[0;32m   1110\u001b[0m         \u001b[38;5;66;03m# safer because np.float64(X*W) != np.float64(X)*np.float64(W)\u001b[39;00m\n\u001b[0;32m   1111\u001b[0m         correction \u001b[38;5;241m=\u001b[39m _safe_accumulator_op(\n\u001b[0;32m   1112\u001b[0m             np\u001b[38;5;241m.\u001b[39mmatmul, sample_weight, np\u001b[38;5;241m.\u001b[39mwhere(X_nan_mask, \u001b[38;5;241m0\u001b[39m, temp)\n\u001b[0;32m   1113\u001b[0m         )\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 765. MiB for an array with shape (25063500, 4) and data type float64"
     ]
    }
   ],
   "source": [
    "# Get unique feature config train set test size combinations\n",
    "\n",
    "unique_configs = set()\n",
    "\n",
    "for model_info in models.values():\n",
    "    feature_config = frozenset(model_info[\"feature_config\"].items())  # dict → frozenset of (key, value)\n",
    "    config = (\n",
    "        feature_config,\n",
    "        model_info[\"time_bucket_folder\"],\n",
    "        model_info[\"test_size\"]\n",
    "    )\n",
    "    unique_configs.add(config)\n",
    "\n",
    "# 2. Now unique_configs contains all unique setups\n",
    "#    You can generate/load your train/test datasets once per config\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "for config in unique_configs:\n",
    "    feature_config_frozen, time_bucket_folder, test_size = config\n",
    "\n",
    "    # Convert frozenset back to dict\n",
    "    features_config = FeaturesConfig(**model_info[\"feature_config\"])\n",
    "\n",
    "    # Generate/load your train/test data\n",
    "    X_train, X_test, y_train, y_test, X_scaler, y_scaler = generate_data(\n",
    "        features_config=features_config,\n",
    "        time_bucket_folder=time_bucket_folder,\n",
    "        test_size=test_size\n",
    "    )\n",
    "\n",
    "    datasets[config] = {\n",
    "        \"X_train\": X_train,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_test\": y_test,\n",
    "        \"X_scaler\": X_scaler,\n",
    "        \"y_scaler\": y_scaler\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dac2e9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m523/523\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 146ms/step\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'y_scaler'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Inverse transform to get real values\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m y_pred_actual \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my_scaler\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39minverse_transform(y_pred)\n\u001b[0;32m     21\u001b[0m y_test_actual \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_scaler\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39minverse_transform(y_test)\n\u001b[0;32m     23\u001b[0m model_results[model_name] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m\"\u001b[39m: y_pred_actual, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreal\u001b[39m\u001b[38;5;124m\"\u001b[39m: y_test_actual}\n",
      "\u001b[1;31mKeyError\u001b[0m: 'y_scaler'"
     ]
    }
   ],
   "source": [
    "# Get results for each model\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "for model_name, model_info in models.items():\n",
    "    config_key = (\n",
    "        frozenset(model_info[\"feature_config\"].items()),\n",
    "        model_info[\"time_bucket_folder\"],\n",
    "        model_info[\"test_size\"]\n",
    "    )\n",
    "\n",
    "    data = datasets[config_key]\n",
    "\n",
    "    X_test = data[\"X_test\"]\n",
    "    y_test = data[\"y_test\"]\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Inverse transform to get real values\n",
    "    y_pred_actual = data[\"y_scaler\"].inverse_transform(y_pred)\n",
    "    y_test_actual = data[\"y_scaler\"].inverse_transform(y_test)\n",
    "\n",
    "    model_results[model_name] = {\"pred\": y_pred_actual, \"real\": y_test_actual}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ef2898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def large_move_metrics(y_true, y_pred, threshold=0.02):\n",
    "    \"\"\"\n",
    "    Computes metrics focused only on large price moves.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: np.ndarray, true target values\n",
    "    - y_pred: np.ndarray, predicted target values\n",
    "    - threshold: float, defines a 'large move' (e.g., 0.02 = 2%)\n",
    "    \n",
    "    Returns:\n",
    "    - metrics: dict with direction accuracy, MSE, MAE, count of large moves\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true).flatten()\n",
    "    y_pred = np.array(y_pred).flatten()\n",
    "\n",
    "    # Select only large moves based on true values\n",
    "    large_moves_idx = np.where(np.abs(y_true) >= threshold)[0]\n",
    "\n",
    "    if len(large_moves_idx) == 0:\n",
    "        print(\"Warning: No large moves found above threshold.\")\n",
    "        return None\n",
    "\n",
    "    y_true_large = y_true[large_moves_idx]\n",
    "    y_pred_large = y_pred[large_moves_idx]\n",
    "\n",
    "    # Directional accuracy (sign matching)\n",
    "    true_sign = np.sign(y_true_large)\n",
    "    pred_sign = np.sign(y_pred_large)\n",
    "    directional_accuracy = np.mean(true_sign == pred_sign)\n",
    "\n",
    "    # MSE and MAE only for large moves\n",
    "    mse = np.mean(np.square(y_true_large - y_pred_large))\n",
    "    mae = np.mean(np.abs(y_true_large - y_pred_large))\n",
    "\n",
    "    metrics = {\n",
    "        'directional_accuracy': directional_accuracy,\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'large_move_count': len(large_moves_idx)\n",
    "    }\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe47da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Let's assume you have already defined the `large_move_metrics` function\n",
    "# and the model_results dictionary where each entry is the predictions and real values for each model\n",
    "\n",
    "# Store the metrics for each model\n",
    "model_comparison = []\n",
    "\n",
    "for model_name, results in model_results.items():\n",
    "    y_pred = results[\"pred\"]\n",
    "    y_real = results[\"real\"]\n",
    "    \n",
    "    # Compute the large move metrics (directional accuracy, etc.)\n",
    "    metrics = large_move_metrics(y_real, y_pred, threshold=0.02)\n",
    "    \n",
    "    # Save the metrics for this model\n",
    "    model_comparison.append({\n",
    "        \"model_name\": model_name,\n",
    "        \"directional_accuracy\": metrics[\"directional_accuracy\"],\n",
    "        \"large_move_mse\": metrics[\"large_move_mse\"]\n",
    "    })\n",
    "\n",
    "# Convert the results into a DataFrame for easy plotting\n",
    "import pandas as pd\n",
    "df_comparison = pd.DataFrame(model_comparison)\n",
    "\n",
    "# Plotting the comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Bar plot for directional accuracy\n",
    "ax.bar(df_comparison[\"model_name\"], df_comparison[\"directional_accuracy\"], width=0.4, label=\"Directional Accuracy\", align=\"center\")\n",
    "\n",
    "# Bar plot for large move MSE\n",
    "ax.bar(df_comparison[\"model_name\"], df_comparison[\"large_move_mse\"], width=0.4, label=\"Large Move MSE\", align=\"edge\")\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel(\"Model\")\n",
    "ax.set_ylabel(\"Metric Value\")\n",
    "ax.set_title(\"Comparison of Models using Large Move Metrics\")\n",
    "ax.legend()\n",
    "\n",
    "# Rotate x-axis labels to make them readable\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CM3203-Venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
