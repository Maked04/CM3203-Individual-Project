{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb765a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a5174dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler  # type: ignore\n",
    "from tensorflow.keras.models import load_model # type: ignore\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Local imports\n",
    "from src.data_processing.lstm_data_preprocessing import reduce_time_bucket_features, FeaturesConfig, TimeBucketConfig\n",
    "from src.data_processing.loader import load_time_bucket_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cf0b3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and details on configurations used to train the model like the time bucket data used and the features used\n",
    "\n",
    "# Custom loss function\n",
    "def weighted_mse_large_moves(y_true, y_pred):\n",
    "    diff = y_true - y_pred\n",
    "    weight = tf.math.square(y_true)\n",
    "    return tf.reduce_mean(weight * tf.square(diff))\n",
    "\n",
    "model_folder = \"../model_generation/trained_models/lstm_1\"\n",
    "model = load_model(os.path.join(model_folder, \"model.keras\"),\n",
    "    custom_objects={'weighted_mse_large_moves': weighted_mse_large_moves})\n",
    "with open(os.path.join(model_folder, \"config.json\"), 'r') as f:\n",
    "    configs = json.load(f)\n",
    "\n",
    "features_config = FeaturesConfig(**configs[\"features_config\"])\n",
    "time_bucket_folder = configs[\"time_bucket_folder\"]\n",
    "test_size = configs[\"test_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abb4ea41",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\matth\\\\Uni Work\\\\CM3203 - Individual Project\\\\Project Environment\\\\CM3203-Individual-Project\\\\data\\\\time_bucket_data\\\\time_bucket_1\\\\config.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m X_scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m      4\u001b[0m y_scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m----> 6\u001b[0m token_time_buckets, time_bucket_config \u001b[38;5;241m=\u001b[39m \u001b[43mload_time_bucket_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_bucket_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m token_datasets \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token_address, data \u001b[38;5;129;01min\u001b[39;00m token_time_buckets\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32m~\\Uni Work\\CM3203 - Individual Project\\Project Environment\\CM3203-Individual-Project\\src\\data_processing\\loader.py:195\u001b[0m, in \u001b[0;36mload_time_bucket_data\u001b[1;34m(folder_name)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_time_bucket_data\u001b[39m(folder_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_bucket_1\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    193\u001b[0m     folder_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(get_data_dir(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_bucket_data\u001b[39m\u001b[38;5;124m\"\u001b[39m, folder_name)\n\u001b[1;32m--> 195\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    196\u001b[0m         config \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m    198\u001b[0m     token_time_buckets \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\matth\\\\Uni Work\\\\CM3203 - Individual Project\\\\Project Environment\\\\CM3203-Individual-Project\\\\data\\\\time_bucket_data\\\\time_bucket_1\\\\config.json'"
     ]
    }
   ],
   "source": [
    "# Get the train test data set used to train the model were testing\n",
    "\n",
    "X_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "\n",
    "token_time_buckets, time_bucket_config = load_time_bucket_data(time_bucket_folder)\n",
    "\n",
    "token_datasets = []\n",
    "for token_address, data in token_time_buckets.items():\n",
    "    X = data[\"X\"]\n",
    "    y = data[\"y\"]\n",
    "    bucket_times = data[\"bucket_times\"]\n",
    "\n",
    "    # Only get the features listed in features_config\n",
    "    X = reduce_time_bucket_features(X, features_config)\n",
    "\n",
    "    token_datasets.append((X, y, token_address, bucket_times))\n",
    "\n",
    "# Combine all token data\n",
    "all_X = np.vstack([data[0] for data in token_datasets])\n",
    "all_y = np.vstack([data[1].reshape(-1, 1) for data in token_datasets])\n",
    "\n",
    "# Scale features\n",
    "num_samples, time_steps, features = all_X.shape\n",
    "X_reshaped = all_X.reshape(num_samples * time_steps, features)\n",
    "X_scaled = X_scaler.fit_transform(X_reshaped)\n",
    "X_scaled = X_scaled.reshape(num_samples, time_steps, features)\n",
    "\n",
    "# Scale target variable also using StandardScaler to preserve direction\n",
    "y_scaled = y_scaler.fit_transform(all_y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=test_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b251f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Inverse transform to get real values\n",
    "y_pred_actual = y_scaler.inverse_transform(y_pred)\n",
    "y_test_actual = y_scaler.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bb396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualisation_methods import plot_error_distribution, plot_directional_accuracy, plot_prediction_vs_actual, plot_magnitude_accuracy, plot_value_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6e73e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_distribution(y_pred_actual, y_test_actual, title=\"Error distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a732d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_value_distributions(y_pred_actual, y_test_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42353a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(y_pred_actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e986e207",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [-np.inf, -8, -4, -2, -1, -0.5, -0.1, 0, 0.1, 0.5, 1, 2, 4, 8, np.inf]\n",
    "plot_directional_accuracy(y_pred_actual, y_test_actual, bins=bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49a682a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction_vs_actual(y_pred_actual, y_test_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c95159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualisation_methods import plot_error_heatmap\n",
    "plot_error_heatmap(y_pred_actual, y_test_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a7a9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_magnitude_accuracy(y_pred_actual, y_test_actual, bins=bins, use_percentage_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e6cc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_tokens_with_large_pred(token_datasets, test_size, y_pred_actual, min_abs_pred_size=1.5):\n",
    "    total_buckets = sum(len(data[1]) for data in token_datasets)\n",
    "    test_start_idx = int((1 - test_size) * total_buckets)\n",
    "\n",
    "    fully_test_tokens = {}\n",
    "    current_idx = 0\n",
    "    y_pred_test_index = 0  # Index within y_pred_actual\n",
    "\n",
    "    for X, y, token_address, bucket_times in token_datasets:\n",
    "        token_len = len(y)\n",
    "        token_start_idx = current_idx\n",
    "        current_idx += token_len\n",
    "\n",
    "        # Only process tokens fully in the test set\n",
    "        if token_start_idx >= test_start_idx:\n",
    "            bucket_pred_map = []\n",
    "\n",
    "            for i in range(token_len):\n",
    "                if y_pred_test_index >= len(y_pred_actual):\n",
    "                    break  # Prevent overflow if mismatch in lengths\n",
    "                pred = y_pred_actual[y_pred_test_index].item()\n",
    "                if abs(pred) >= min_abs_pred_size:\n",
    "                    bucket_pred_map.append({\n",
    "                        'bucket_time': tuple(bucket_times[i]),\n",
    "                        'prediction': pred\n",
    "                    })\n",
    "                y_pred_test_index += 1\n",
    "\n",
    "            if bucket_pred_map:\n",
    "                fully_test_tokens[token_address] = bucket_pred_map\n",
    "\n",
    "    return fully_test_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a05ecde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set minimum absolute prediction size\n",
    "min_abs_pred_size = 1.5\n",
    "\n",
    "# Get tokens with large predictions\n",
    "test_tokens_pred = get_test_tokens_with_large_pred(token_datasets, test_size, y_pred_actual, min_abs_pred_size=min_abs_pred_size)\n",
    "\n",
    "# Print the results\n",
    "print(list(test_tokens_pred.keys()))  # Print token addresses\n",
    "print(len(test_tokens_pred))  # Print number of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e56e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualisation_methods import plot_predictions_on_price_graph\n",
    "token_address = \"bpR2DF4sMarWvMJT6tevpQ9K7AtXskBQ6zA35iCpump\"\n",
    "bucket_pred_map = test_tokens_pred[token_address]\n",
    "plot_predictions_on_price_graph(token_address, bucket_pred_map, min_abs_pred_size=min_abs_pred_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27da2998",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_abs_pred_size = 3\n",
    "test_tokens_real = get_test_tokens_with_large_pred(token_datasets, test_size, y_test_actual, min_abs_pred_size=min_abs_pred_size)\n",
    "print(list(test_tokens_real.keys()))\n",
    "print(len(test_tokens_real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce040351",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_address = \"E5YYfMPBbWz3vvaQcdPfNSC659nBmMZFqpAXqPtcpump\"\n",
    "bucket_pred_map = test_tokens_real[token_address]\n",
    "plot_predictions_on_price_graph(token_address, bucket_pred_map, min_abs_pred_size=min_abs_pred_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8535de8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_processing.loader import load_token_price_data\n",
    "\n",
    "# Get all 1.75x predictions and their tokens\n",
    "test_tokens_pred = get_test_tokens_with_large_pred(token_datasets, test_size, y_pred_actual, min_abs_pred_size=1.5)\n",
    "\n",
    "# Load price data\n",
    "tokens_tx_data = {\n",
    "    token: load_token_price_data(token, use_datetime=False)\n",
    "    for token in test_tokens_pred.keys()\n",
    "}\n",
    "\n",
    "buy_size = 0.1  # Fixed investment in SOL\n",
    "\n",
    "# Strategy parameters\n",
    "sell_targets = [1.0, 1.5, 2.5, 5]                # Profit target percentages (e.g., +100%, +150%)\n",
    "trailing_stop_targets = [0.5, 0.75]       # Trailing drawdown levels (e.g., -10%, -20%)\n",
    "buy_fee_pct = 0.01\n",
    "sell_fee_pct = 0.01\n",
    "slippage_pct = 0.005\n",
    "\n",
    "# Stats\n",
    "total_profit = 0\n",
    "num_trades = 0\n",
    "num_wins = 0\n",
    "num_losses = 0\n",
    "profits = []\n",
    "\n",
    "for token_address, bucket_pred_map in test_tokens_pred.items():\n",
    "    price_df = tokens_tx_data[token_address]\n",
    "    price_df = price_df.groupby(price_df.index).mean().sort_index()\n",
    "\n",
    "    print(f\"\\nSimulating for token: {token_address}\")\n",
    "    executed_trade = False\n",
    "\n",
    "    for pred_data in bucket_pred_map:\n",
    "        if executed_trade:\n",
    "            break\n",
    "\n",
    "        bucket_time = pred_data['bucket_time']\n",
    "        pred = pred_data['prediction']\n",
    "        if pred < 0 :\n",
    "            continue\n",
    "        start_time, end_time = bucket_time\n",
    "\n",
    "        if end_time not in price_df.index:\n",
    "            closest_idx = price_df.index.get_indexer([end_time], method='nearest')[0]\n",
    "            closest_time = price_df.index[closest_idx]\n",
    "        else:\n",
    "            closest_time = end_time\n",
    "\n",
    "        raw_buy_price = price_df.loc[closest_time, 'price']\n",
    "        if pd.isna(raw_buy_price):\n",
    "            continue\n",
    "\n",
    "        buy_price = raw_buy_price * (1 + slippage_pct)\n",
    "        entry_value = buy_size * (1 - buy_fee_pct)\n",
    "        tokens_bought = entry_value / buy_price\n",
    "        remaining_tokens = tokens_bought\n",
    "        profit = -buy_size\n",
    "        peak_price = buy_price\n",
    "\n",
    "        print(f\"  Prediction: {pred}x | Buy Price: {buy_price} (with slippage/fees)\")\n",
    "\n",
    "        future_prices = price_df.loc[closest_time:]\n",
    "        if len(future_prices) < 2:\n",
    "            continue\n",
    "\n",
    "        target_idx = 0\n",
    "        stop_idx = 0\n",
    "\n",
    "        for time, row in future_prices.iterrows():\n",
    "            raw_price = row['price']\n",
    "            peak_price = max(peak_price, raw_price)\n",
    "\n",
    "            sell_price = raw_price * (1 - slippage_pct) * (1 - sell_fee_pct)\n",
    "\n",
    "            # Profit targets\n",
    "            if target_idx < len(sell_targets) and raw_price >= buy_price * (1 + sell_targets[target_idx]):\n",
    "                sell_amount = remaining_tokens * 0.5\n",
    "                profit += sell_amount * sell_price\n",
    "                remaining_tokens -= sell_amount\n",
    "                print(f\"    Sold 50% at profit target {sell_targets[target_idx]}x | Price: {raw_price}\")\n",
    "                target_idx += 1\n",
    "\n",
    "            # Trailing stop stages\n",
    "            drawdown = 1 - (raw_price / peak_price)\n",
    "            if stop_idx < len(trailing_stop_targets) and drawdown >= trailing_stop_targets[stop_idx]:\n",
    "                sell_amount = remaining_tokens * 0.5\n",
    "                profit += sell_amount * sell_price\n",
    "                remaining_tokens -= sell_amount\n",
    "                print(f\"    Sold 50% at trailing stop {int(trailing_stop_targets[stop_idx]*100)}% | Price: {raw_price}\")\n",
    "                stop_idx += 1\n",
    "\n",
    "            # Fully exit if no tokens left\n",
    "            if remaining_tokens <= 0:\n",
    "                print(f\"    Fully exited. Total profit: {profit} SOL\")\n",
    "                break\n",
    "\n",
    "        # If tokens remain, sell at final price\n",
    "        if remaining_tokens > 0:\n",
    "            final_price = raw_price * (1 - slippage_pct) * (1 - sell_fee_pct)\n",
    "            profit += remaining_tokens * final_price\n",
    "            print(f\"    Final exit at {raw_price:.4f}. Total profit: {profit} SOL\")\n",
    "\n",
    "        # Update stats\n",
    "        executed_trade = True\n",
    "        num_trades += 1\n",
    "        total_profit += profit\n",
    "        profits.append(profit)\n",
    "        if profit > 0:\n",
    "            num_wins += 1\n",
    "        else:\n",
    "            num_losses += 1\n",
    "\n",
    "# Summary\n",
    "print(\"\\n--- Strategy Summary ---\")\n",
    "print(f\"Total Trades: {num_trades}\")\n",
    "print(f\"Wins: {num_wins} | Losses: {num_losses}\")\n",
    "print(f\"Win Rate: {(num_wins / num_trades * 100):.2f}%\" if num_trades > 0 else \"No trades made\")\n",
    "print(f\"Total Profit: {total_profit:.4f} SOL\")\n",
    "if num_trades > 0:\n",
    "    print(f\"Average Profit per Trade: {(total_profit / num_trades):.4f} SOL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8f5b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Assuming your model is named 'model'\n",
    "plot_model(model, to_file='model_diagram.png', show_shapes=True, show_layer_names=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CM3203-Venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
